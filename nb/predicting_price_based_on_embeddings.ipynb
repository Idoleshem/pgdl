{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc6171-a93c-4fc5-bb8d-7f63daa2aab5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import json, requests, time\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('postgresql://postgres:argmax@pg:5432/postgres')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c6725",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Every time a user opens a mobile app, an auction is going on behind the scenes. The highest bidder gets to advertise his ad to the user.\n",
    "\n",
    "## Auctions Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a63fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = 'SELECT * FROM auctions;'\n",
    "with engine.connect() as db_con:\n",
    "    df = pd.read_sql(sql_query, con=db_con)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c86e9",
   "metadata": {},
   "source": [
    "## App Vectors table\n",
    "\n",
    "We've gathered the first few sentences from the app store description and embedded it with a [model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05408c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f'''\n",
    "SELECT\n",
    "    *\n",
    "FROM app_vectors\n",
    "'''\n",
    "has_embedding = False\n",
    "while not has_embedding:\n",
    "    with engine.connect() as db_con:\n",
    "        df_of_embeddings = pd.read_sql(sql_query, con=db_con)\n",
    "    has_embedding = (~df_of_embeddings[\"embedding\"].isna()).all()\n",
    "    if not has_embedding:\n",
    "        print(\"Waiting for embeddings...\")\n",
    "        time.sleep(15)\n",
    "\n",
    "df_of_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac0d2f",
   "metadata": {},
   "source": [
    "We can use the `<=>` operator to run vector search within the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504473",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = json.loads(df_of_embeddings.embedding[0]) # get the first embedding\n",
    "print (\"Embedding size: {l}\".format(l=len(vec)))\n",
    "\n",
    "sql_query = f'''\n",
    "SELECT\n",
    "    \"bundleId\"\n",
    "FROM app_vectors\n",
    "ORDER BY embedding<=>'{json.dumps(vec)}'\n",
    "'''\n",
    "with engine.connect() as db_con:\n",
    "    similar_embeddings = pd.read_sql(sql_query, con=db_con)\n",
    "\n",
    "similar_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7be478",
   "metadata": {},
   "source": [
    "# What you need to do\n",
    "\n",
    "## The hypothesis\n",
    "\n",
    "We assume that apps with similar desciptions, would have a similar asking price in the auctions (`sentPrice` column).\n",
    "\n",
    "Use cosine similarity (`<=>`) on the embeddings to find similar apps, and any statistical tools you find suitable to prove or disprove this hypothesis.\n",
    "\n",
    "## Is it consistent?\n",
    "\n",
    "There are several other features in the auctions table (such as `CountryCode` and `OS`),\n",
    "Do your findings hold for those as well?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b61a69-83f0-4325-aeb7-9c0bfe70a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's install some additional libraries in order to visualize the data and compare different models\n",
    "!pip install scikit-learn\n",
    "!pip install xgboost\n",
    "!pip install seaborn\n",
    "\n",
    "# import the required libraries\n",
    "import itertools\n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "import seaborn as sns # for data visualization\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity # for computing cosine similarity\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into training, validation and test sets\n",
    "\n",
    "# will use us for selecting random apps to put in the validation and test sets\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac47c2",
   "metadata": {},
   "source": [
    "# ------------------- Planning -------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de00c91",
   "metadata": {},
   "source": [
    "# My work plan for testing the hypotesis:\n",
    "\n",
    "(1) Start with some descriptive statistics in order to better understand the data and what decsisions to make\n",
    "\n",
    "(2) Evalute the quality of the embeddings, I would assume that apps with similar semantic description would have similar vector embeddings. If it's not the case, maybe we should use different embedding model.\n",
    "\n",
    "(3) Design an experiment which compares a baseline model which was trained without embeddings and a model which takes into account the apps' embeddings\n",
    "\n",
    "(4) Execution - Feature engineering, splitting the data in a way which avoids data leakage, training\n",
    "\n",
    "(5) Results\n",
    "\n",
    "(6) Testing the contribution of additional features\n",
    "\n",
    "(7) Conclusions\n",
    "\n",
    "For that, we can create a confusion matrix which contains the cosine similarity values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ec99c",
   "metadata": {},
   "source": [
    "# ------------------- (1) Descriptive statistics -------------------\n",
    "\n",
    "Let's analyze the sentPrice values with respect to each app.\n",
    "\n",
    "This will enable us to understand the number of apps present, their sentPrice distribution, and their percentage out of all the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I Group by the df based on the 'bundleId' column and calculate descriptive statistics for 'sentPrice'\n",
    "grouped_stats = df.groupby('bundleId')['sentPrice'].describe()\n",
    "\n",
    "# Calculate percentage of each group out of all records\n",
    "grouped_stats['percentage'] = grouped_stats['count'] / len(df) * 100\n",
    "\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ca836",
   "metadata": {},
   "source": [
    "# Notes from the descriptive analysis:\n",
    "\n",
    "It seems that some apps have less than 5% of the samples.\n",
    "\n",
    "We should keep some apps for the validation and test data, in order to make sure the embeddings of these apps would not leak in to training data.\n",
    "\n",
    "More about it later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d041e",
   "metadata": {},
   "source": [
    "# ------------------- (2) Embeddings qualitative evaluation -------------------\n",
    "\n",
    "Based on the pervious analysis, it seems that there are only 18 apps.\n",
    "\n",
    "Let's create a confusion matrix in order to easily examine the similarity between the apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0672db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that computes the cosine similarity between two embeddings\n",
    "def create_cos_sim_df(df_of_embeddings):\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    # (1) Convert string representations of lists to actual lists\n",
    "    df_of_embeddings['embedding'] = df_of_embeddings['embedding'].apply(json.loads)\n",
    "\n",
    "    # store embeddings in a dict:\n",
    "    for idx, row in df_of_embeddings.iterrows():\n",
    "        embeddings_dict[row['bundleId']] = row['embedding']\n",
    "        \n",
    "    # (2) Initialize a matrix to store cosine similarities\n",
    "    num_embeddings = len(df_of_embeddings)\n",
    "    cos_sim_matrix = np.zeros((num_embeddings, num_embeddings)) \n",
    "\n",
    "    # (3) Compute cosine similarity between each pair of embeddings\n",
    "    for i, j in itertools.product(range(num_embeddings), range(num_embeddings)):\n",
    "        # ignore the diagonal elements\n",
    "        if i != j: \n",
    "            cos_sim_matrix[i, j] = cosine_similarity([df_of_embeddings['embedding'][i]], [df_of_embeddings['embedding'][j]])[0, 0]\n",
    "\n",
    "    # (4) reate DataFrame for cosine similarity matrix\n",
    "    cos_sim_df = pd.DataFrame(cos_sim_matrix, columns=df_of_embeddings['bundleId'], index=df_of_embeddings['bundleId'])\n",
    "\n",
    "    return cos_sim_df,embeddings_dict\n",
    "\n",
    "cos_sim_df,embeddings_dict = create_cos_sim_df(df_of_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02711ae6",
   "metadata": {},
   "source": [
    "## Visualize confusion matrix which reflects the cosine similarity values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of cosine similarity values\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cos_sim_df, annot=True, cmap='coolwarm')\n",
    "plt.title('Cosine Similarity Heatmap')\n",
    "plt.xlabel('bundleId')\n",
    "plt.ylabel('bundleId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb0cfa",
   "metadata": {},
   "source": [
    "## Assessing Similarity of Apps Based on Cosine Similarity Values\n",
    "\n",
    "For example, consider the apps \"com.volt.dresstoimpress\" and \"1569586264\" with a cosine similarity of 1.\n",
    "\n",
    "The app description for \"com.volt.dresstoimpress\" is: \"Choose the appropriate outfit to make it through different social events!\"\n",
    "\n",
    "The app description for \"1569586264\" is: \"Choose the appropriate outfit to make it through different social events!\"\n",
    "\n",
    "This analysis suggests that these might be the same apps!\n",
    "\n",
    "---\n",
    "\n",
    "## Let's also analyze two embeddings that are not identical but have a high cosine similarity of 0.92.\n",
    "\n",
    "The app description for \"com.loop.match3d\" is: \"Get ready for a new, challenging and original matching pairs brain game.\"\n",
    "\n",
    "The app description for \"1502447854\" is: \"Get ready for a new, challenging and original matching pairs game.\"\n",
    "\n",
    "These descriptions are very close!\n",
    "\n",
    "---\n",
    "\n",
    "## Lastly, let's examine a couple of apps with medium similarity = 0.5.\n",
    "\n",
    "The app description for \"com.tintash.nailsalon\" is:\n",
    "\n",
    "\"It is manicure madness over here and it’s your time to become the greatest Nail Salon of 2021! All you need to do is scrape, clip, paint, polish and perfect your client’s nails and you will be raking in the money in no time! Just don’t mess up! People don’t like when you accidentally pull their nails off. Ouch!\"\n",
    "\n",
    "The app description for \"com.volt.dresstoimpress\" is:\n",
    "\n",
    "\"Choose the appropriate outfit to make it through different social events!\".\n",
    "\n",
    "Conclusion - not so similar.\n",
    "\n",
    "# Final conclusion - the embeddings seems to reflect the semantical similarity between apps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09a5fe",
   "metadata": {},
   "source": [
    "# ------------------- (3) Experiment design -------------------\n",
    "\n",
    "- I would suggest training a baseline model aimed at predicting the sentPrice target variable.\n",
    "  Input - bids' timestamp.\n",
    "  Output - sentPrice.\n",
    "\n",
    "- The baseline model would be compared with a model trained on additional features that reflect the app embeddings.\n",
    "  Input - bids' timestamp, embeddings (after some standard clustering).\n",
    "  Output - sentPrice.\n",
    "\n",
    "- The model which will use me is the popular XGBOOST - this model known for it's powerful predictive and can also provide us insights regarding the feature importance\n",
    "  Evaluation metric - RMSE.\n",
    "\n",
    "- If the baseline_RMSE < model_with_embedding_RMSE, then the hypothesis would be confirmed.\n",
    "\n",
    "# note\n",
    "\n",
    "- Since there are some apps with low represntation, I would suggest randomly assign some of them to the validation and testing sets.\n",
    "  This would prevent data leakage when performing the embeddings clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011362c5",
   "metadata": {},
   "source": [
    "# ------------------- (4) Execution -------------------\n",
    "\n",
    "This phase includes:\n",
    "\n",
    "(a) Feature engineering - I will transform the timestamp variable into year,month, day, hour, minute,second and day_of_week.\n",
    "\n",
    "(b) I will randomly some of the low appearance apps to the validation and testing data\n",
    "\n",
    "(c) Training a model without the apps' embeddings\n",
    "\n",
    "(d) Clustering the apps based on cosine similarity + generate one hot encoding feature for each cluster\n",
    "\n",
    "(e) Train a model with the app's embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6b489",
   "metadata": {},
   "source": [
    "# 4a. Split the data into training, validation and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from the eventTimestamp variable\n",
    "\n",
    "def timestamp_to_features(df):\n",
    "    # Create features from the eventTimestamp variable\n",
    "    df['eventTimestamp'] = pd.to_numeric(df['eventTimestamp'], errors='coerce')\n",
    "    df['eventTimestamp'] = pd.to_datetime(df['eventTimestamp'] / 1000, unit='s')\n",
    "\n",
    "    # Extract year, month, day, hour, minute, second, and day of the week\n",
    "    df['year'] = df['eventTimestamp'].dt.year\n",
    "    df['month'] = df['eventTimestamp'].dt.month\n",
    "    df['day'] = df['eventTimestamp'].dt.day\n",
    "    df['hour'] = df['eventTimestamp'].dt.hour\n",
    "    df['minute'] = df['eventTimestamp'].dt.minute\n",
    "    df['second'] = df['eventTimestamp'].dt.second\n",
    "    df['day_of_week'] = df['eventTimestamp'].dt.dayofweek \n",
    "\n",
    "    # Drop the original eventTimestamp column\n",
    "    df = df.drop(columns=['eventTimestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = timestamp_to_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b41b894",
   "metadata": {},
   "source": [
    "# 4.b Randomly assign 2 unique apps for the validation and 2 for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea22b501",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grouped_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     test_apps \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(low_representation_apps, apps_for_test)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validation_apps, test_apps\n\u001b[0;32m---> 22\u001b[0m validation_apps, test_apps \u001b[38;5;241m=\u001b[39m select_apps_for_validation_and_test(\u001b[43mgrouped_stats\u001b[49m, apps_for_validation, apps_for_test)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print the selected apps\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation apps: \u001b[39m\u001b[38;5;124m\"\u001b[39m, validation_apps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grouped_stats' is not defined"
     ]
    }
   ],
   "source": [
    "# hyperparameters for picking the apps for validation and test sets\n",
    "low_appearance_apps_percantage_threshold = 5 # 5%\n",
    "apps_for_validation = 2\n",
    "apps_for_test = 2\n",
    "\n",
    "def select_apps_for_validation_and_test(grouped_stats, apps_for_validation, apps_for_test):\n",
    "\n",
    "    # Select apps that have a low appearance percentage\n",
    "    low_representation_apps = grouped_stats[grouped_stats[\"percentage\"]<low_appearance_apps_percantage_threshold]\n",
    "\n",
    "    # Make a list of the selected apps\n",
    "    low_representation_apps = low_representation_apps.index.tolist()\n",
    "\n",
    "    # Shuffle list:\n",
    "    random.shuffle(low_representation_apps)\n",
    "\n",
    "    # Select random apps for validation and test sets\n",
    "    validation_apps = low_representation_apps[:apps_for_validation]\n",
    "    test_apps = low_representation_apps[apps_for_validation: apps_for_validation + apps_for_test]\n",
    "\n",
    "    return validation_apps, test_apps\n",
    "\n",
    "validation_apps, test_apps = select_apps_for_validation_and_test(grouped_stats, apps_for_validation, apps_for_test)\n",
    "\n",
    "# print the selected apps\n",
    "print(\"Validation apps: \", validation_apps)\n",
    "print(\"Test apps: \", test_apps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f304d",
   "metadata": {},
   "source": [
    "# 4.c Split data for training, validationa and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_for_training(df,columns_to_use,validation_apps,test_apps):\n",
    "    \n",
    "    # Split the dataset into 80% training and 20% temporary sets\n",
    "    X_train_temp, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Further split the temporary set into 70% training and 10% validation sets\n",
    "    X_train, X_val = train_test_split(X_train_temp, test_size=0.125, random_state=42)\n",
    "\n",
    "    # move the validation and test apps to the validation and test sets\n",
    "    # Filter records associated with validation apps from the training set\n",
    "    train_data_validation_apps = X_train[X_train['bundleId'].isin(validation_apps)]\n",
    "    X_train = X_train[~X_train['bundleId'].isin(validation_apps)]\n",
    "    X_val = pd.concat([X_val, train_data_validation_apps])\n",
    "\n",
    "    # Filter records associated with test apps from the training set\n",
    "    train_data_test_apps = X_train[X_train['bundleId'].isin(test_apps)]\n",
    "    X_train = X_train[~X_train['bundleId'].isin(test_apps)]\n",
    "    X_test = pd.concat([X_test, train_data_test_apps])\n",
    "\n",
    "    # Shuffling the set to ensure randomness\n",
    "    X_train = X_train.sample(frac=1, random_state=42)\n",
    "    X_val = X_val.sample(frac=1, random_state=42)\n",
    "    X_test = X_test.sample(frac=1, random_state=42)\n",
    "\n",
    "    # Extract the target \n",
    "    y_train = X_train.pop('sentPrice')\n",
    "    y_val = X_val.pop('sentPrice')\n",
    "    y_test = X_test.pop('sentPrice')\n",
    "\n",
    "    # Calculate the proportion of each set\n",
    "    train_proportion = len(X_train) / len(df)\n",
    "    val_proportion = len(X_val) / len(df)\n",
    "    test_proportion = len(X_test) / len(df)\n",
    "\n",
    "    print(\"Proportion of training data:\", round(train_proportion,4)*100)\n",
    "    print(\"Proportion of validation data:\", round(val_proportion,4)*100)\n",
    "    print(\"Proportion of test data:\", round(test_proportion,4)*100)\n",
    "\n",
    "    # select only the columns_to_use for the training, validation and testing \n",
    "    X_train = X_train[columns_to_use]\n",
    "    X_val = X_val[columns_to_use]\n",
    "    X_test = X_test[columns_to_use]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Select specific columns for training\n",
    "columns_to_use = ['bidFloorPrice', 'year', 'month', 'day', 'hour', 'minute', 'second', 'day_of_week']\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data_for_training(df,columns_to_use,validation_apps, test_apps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a6001",
   "metadata": {},
   "source": [
    "# Train the XGBOOST model without considering the apps' embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \n",
    "    # Init the XGBoost model\n",
    "    model = xgb.XGBRegressor()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    # Evaluate the model on training set\n",
    "    training_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    print(\"Training Root Mean Squared Error:\", training_rmse)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "    print(\"Validation Root Mean Squared Error:\", val_rmse)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on test set\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    print(\"Test Root Mean Squared Error:\", test_rmse)\n",
    "\n",
    "    return model, training_rmse, val_rmse, test_rmse\n",
    "\n",
    "model, training_rmse, val_rmse, test_rmse = train_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583f199",
   "metadata": {},
   "source": [
    "# 4.d - Clustering the apps' embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c14800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_apps_by_similarity(cos_sim_df, apps_to_exclude):\n",
    "    threshold_for_similarity = 0.7\n",
    "    \n",
    "    # Filter rows and columns based on the apps to exclude\n",
    "    cos_sim_df = cos_sim_df.drop(columns=apps_to_exclude, index=apps_to_exclude)\n",
    "\n",
    "    clusters = {}  # Dict to store clusters\n",
    "    cluster_num = 0\n",
    "    checked_apps = []\n",
    "    for current_app, row in cos_sim_df.iterrows():\n",
    "        if current_app in checked_apps:\n",
    "            continue\n",
    "        \n",
    "        max_similarity_app = row.idxmax()\n",
    "        max_similarity_value = row.max()\n",
    "\n",
    "        if max_similarity_value >= threshold_for_similarity:\n",
    "            # Check if the app with the highest similarity is already in a cluster\n",
    "            assigned_cluster = None\n",
    "            for c_num, cluster_data in clusters.items():\n",
    "                if max_similarity_app in cluster_data['apps']:\n",
    "                    assigned_cluster = c_num\n",
    "                    break\n",
    "            \n",
    "            # Add the current app to the cluster\n",
    "            if assigned_cluster is not None:\n",
    "                clusters[assigned_cluster]['apps'].append(current_app)\n",
    "                checked_apps.append(current_app)\n",
    "            \n",
    "            # Create a new cluster\n",
    "            if assigned_cluster is None:\n",
    "                cluster_num += 1\n",
    "                clusters[cluster_num] = {'apps': [current_app, max_similarity_app]}\n",
    "                checked_apps.extend([current_app, max_similarity_app])\n",
    "                \n",
    "\n",
    "    for c_num, cluster_data in clusters.items():\n",
    "        apps_in_cluster = cluster_data['apps']\n",
    "        \n",
    "        # Get embeddings for apps in the cluster from the embeddings_dict\n",
    "        cluster_embeddings = [embeddings_dict[app] for app in apps_in_cluster]\n",
    "        \n",
    "        # Calculate the average embedding for the cluster\n",
    "        cluster_data['average_embedding'] = np.mean(cluster_embeddings, axis=0)\n",
    "    return clusters\n",
    "\n",
    "apps_to_exclude = validation_apps + test_apps\n",
    "clusters = cluster_apps_by_similarity(cos_sim_df,apps_to_exclude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4b8a3",
   "metadata": {},
   "source": [
    "# Assign new apps to the existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_apps_to_clusters(apps_to_exclude, clusters, embeddings_dict):\n",
    "    # Select the best cluster for the apps which were assigned to the test and validation sets\n",
    "    for app in apps_to_exclude:\n",
    "        # Get embedding for the app\n",
    "        current_app_embedding = embeddings_dict[app] \n",
    "        # Calculate cosine similarity with average embeddings of each cluster\n",
    "        similarities = {}\n",
    "        for cluster_num, cluster_data in clusters.items():\n",
    "            cluster_embedding = cluster_data['average_embedding']\n",
    "            similarity = cosine_similarity([current_app_embedding], [cluster_embedding])[0][0]\n",
    "            similarities[cluster_num] = similarity\n",
    "\n",
    "        # Assign the app to the cluster with the highest cosine similarity\n",
    "        max_similarity_cluster = max(similarities, key=similarities.get)\n",
    "        clusters[max_similarity_cluster]['apps'].append(app)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "clusters = assign_apps_to_clusters(apps_to_exclude, clusters, embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1bb59",
   "metadata": {},
   "source": [
    "# Create one hot encoding for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encoding(df, clusters):\n",
    "    \n",
    "    # iterate over the clusters and create one-hot encoding for the apps\n",
    "    for cluster_num, cluster_data in clusters.items():\n",
    "        cluster_apps = cluster_data['apps'] # get the apps in the cluster\n",
    "        cluster_name = f'cluster_{cluster_num}' # create the cluster name\n",
    "        df[cluster_name] = 0 # initialize the column with zeros\n",
    "        df.loc[df['bundleId'].isin(cluster_apps), cluster_name] = 1 # assign 1 to the apps in the cluster\n",
    "\n",
    "        # return also a list of cluster_name\n",
    "        cluster_names = [f'cluster_{cluster_num}' for cluster_num in clusters.keys()]\n",
    "    \n",
    "    return df,cluster_names\n",
    "\n",
    "df,cluster_names = create_one_hot_encoding(df, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1b5e0",
   "metadata": {},
   "source": [
    "# Train a model based on the clusters\n",
    "Let's repeat the train, test, validation process. This time adding the clusters one hot encoding features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ad6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data \n",
    "columns_to_use = columns_to_use.extend(cluster_names)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data_for_training(df,columns_to_use,validation_apps,test_apps)\n",
    "\n",
    "# train the model\n",
    "model, training_rmse, val_rmse, test_rmse = train_model(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3ccad",
   "metadata": {},
   "source": [
    "(7) Conclusions\n",
    "It was found that ...\n",
    "* It's worth examining additional models except XGBOOST\n",
    "* Maybe the threshold of clustering the embeding wasn't high enough\n",
    "* Maybe I should use a different method which is not rely on embedings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
