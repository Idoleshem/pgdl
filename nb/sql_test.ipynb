{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acc6171-a93c-4fc5-bb8d-7f63daa2aab5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import json, requests, time\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('postgresql://postgres:argmax@pg:5432/postgres')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c6725",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Every time a user opens a mobile app, an auction is going on behind the scenes. The highest bidder gets to advertise his ad to the user.\n",
    "\n",
    "## Auctions Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a63fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = 'SELECT * FROM auctions;'\n",
    "with engine.connect() as db_con:\n",
    "    df = pd.read_sql(sql_query, con=db_con)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c86e9",
   "metadata": {},
   "source": [
    "## App Vectors table\n",
    "\n",
    "We've gathered the first few sentences from the app store description and embedded it with a [model](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05408c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f'''\n",
    "SELECT\n",
    "    *\n",
    "FROM app_vectors\n",
    "'''\n",
    "has_embedding = False\n",
    "while not has_embedding:\n",
    "    with engine.connect() as db_con:\n",
    "        df_of_embeddings = pd.read_sql(sql_query, con=db_con)\n",
    "    has_embedding = (~df_of_embeddings[\"embedding\"].isna()).all()\n",
    "    if not has_embedding:\n",
    "        print(\"Waiting for embeddings...\")\n",
    "        time.sleep(15)\n",
    "\n",
    "df_of_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac0d2f",
   "metadata": {},
   "source": [
    "We can use the `<=>` operator to run vector search within the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec = json.loads(df_of_embeddings.embedding[0]) # get the first embedding\n",
    "print (\"Embedding size: {l}\".format(l=len(vec)))\n",
    "\n",
    "sql_query = f'''\n",
    "SELECT\n",
    "    \"bundleId\"\n",
    "FROM app_vectors\n",
    "ORDER BY embedding<=>'{json.dumps(vec)}'\n",
    "'''\n",
    "with engine.connect() as db_con:\n",
    "    similar_embeddings = pd.read_sql(sql_query, con=db_con)\n",
    "\n",
    "similar_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7be478",
   "metadata": {},
   "source": [
    "# What you need to do\n",
    "\n",
    "## The hypothesis\n",
    "\n",
    "We assume that apps with similar desciptions, would have a similar asking price in the auctions (`sentPrice` column).\n",
    "\n",
    "Use cosine similarity (`<=>`) on the embeddings to find similar apps, and any statistical tools you find suitable to prove or disprove this hypothesis.\n",
    "\n",
    "## Is it consistent?\n",
    "\n",
    "There are several other features in the auctions table (such as `CountryCode` and `OS`),\n",
    "Do your findings hold for those as well?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b61a69-83f0-4325-aeb7-9c0bfe70a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's install some additional libraries in order to visualize the data and compare different models\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "\n",
    "# import the necessary libraries\n",
    "import numpy as np \n",
    "import itertools\n",
    "import seaborn as sns # for data visualization\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics.pairwise import cosine_similarity # for computing cosine similarity\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into training, validation and test sets\n",
    "\n",
    "# will use us for selecting random apps to put in the validation and test sets\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de00c91",
   "metadata": {},
   "source": [
    "## embeddings evaluation ##\n",
    "First - let's evalute the quality of the embedding, I would assume that similar apps would have similar embeddings. \n",
    "\n",
    "For that, we can create a confusion matrix which contains the cosine similarity values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0672db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that computes the cosine similarity between two embeddings\n",
    "def create_cos_sim_df(df_of_embeddings):\n",
    "    \n",
    "    # (1) Convert string representations of lists to actual lists\n",
    "    df_of_embeddings['embedding'] = df_of_embeddings['embedding'].apply(json.loads)\n",
    "\n",
    "    # (2) Initialize a matrix to store cosine similarities\n",
    "    num_embeddings = len(df_of_embeddings)\n",
    "    cos_sim_matrix = np.zeros((num_embeddings, num_embeddings)) \n",
    "\n",
    "    # (3) Compute cosine similarity between each pair of embeddings\n",
    "    for i, j in itertools.product(range(num_embeddings), range(num_embeddings)):\n",
    "        # ignore the diagonal elements\n",
    "        if i != j: \n",
    "            cos_sim_matrix[i, j] = cosine_similarity([df_of_embeddings['embedding'][i]], [df_of_embeddings['embedding'][j]])[0, 0]\n",
    "\n",
    "    # (4) reate DataFrame for cosine similarity matrix\n",
    "    cos_sim_df = pd.DataFrame(cos_sim_matrix, columns=df_of_embeddings['bundleId'], index=df_of_embeddings['bundleId'])\n",
    "\n",
    "    return cos_sim_df\n",
    "\n",
    "cos_sim_df = create_cos_sim_df(df_of_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02711ae6",
   "metadata": {},
   "source": [
    "## Visualize confusion matrix which reflects the cosine similarity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of cosine similarity values\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cos_sim_df, annot=True, cmap='coolwarm')\n",
    "plt.title('Cosine Similarity Heatmap')\n",
    "plt.xlabel('bundleId')\n",
    "plt.ylabel('bundleId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb0cfa",
   "metadata": {},
   "source": [
    "## Conducting subjective assessment of similar apps based on their cosine similarty.\n",
    "for example - com.volt.dresstoimpress and 1569586264 got cosine similarity = 1\n",
    "\n",
    "The com.volt.dresstoimpress app content is \"Choose the appropriate outfit to make it through different social events!\"\n",
    "\n",
    "The 1569586264 app content is \"Choose the appropriate outfit to make it through different social events!\"\n",
    "\n",
    "This analysis suggest that these might be the same apps!\n",
    "\n",
    "## ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Let's also anaylse 2 embeddings which are not identical but their cosine similiarty is high = 0.92\n",
    "The com.loop.match3d app content is \"Get ready for a new, challenging and original matching pairs brain game\"\n",
    "\n",
    "The 1502447854 app content is \"\"Get ready for a new, challenging and original matching pairs game.\n",
    "\n",
    "These descriptions are really close!\n",
    "\n",
    "## ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Lastly, let's pick a couple of apps with medium similiarty = 0.5\n",
    "The com.tintash.nailsalon app content is:\n",
    "\n",
    "\"It is manicure madness over here and it�s your time to become the greatest Nail Salon of 2021! All you need to do is scrape, clip, paint, polish and perfect your client�s nails and you will be raking in the money in no time! \n",
    "\n",
    "Just don�t mess up! People don�t like when you accidentally pull their nails off. Ouch!\"\n",
    "\n",
    "the com.volt.dresstoimpress app content is:\n",
    "\n",
    "\"Choose the appropriate outfit to make it through different social events!\"\n",
    "\n",
    "Conclusion - not so similar \n",
    "\n",
    "## ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Final conclusion - the embeddings seems to reflect the semantical similarity between apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1e0da",
   "metadata": {},
   "source": [
    "# Next - Let's analyze the sentPrice values with resepct to each app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets learn more about our data. We can check the apps percenatge of records and the descriptives\n",
    "# Group by 'bundleId' and calculate descriptive statistics for 'sentPrice'\n",
    "grouped_stats = df.groupby('bundleId')['sentPrice'].describe()\n",
    "\n",
    "# Calculate percentage of each group out of all records\n",
    "grouped_stats['percentage'] = grouped_stats['count'] / len(df) * 100\n",
    "\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3740a",
   "metadata": {},
   "source": [
    "# Notes from this analysis:\n",
    "It seems that some apps have less than 5% of the samples => \n",
    "We should put some apps in the validation and test data only in order to make sure the embeddings of these apps would not leak in to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_appearance_apps_threshold = 5\n",
    "low_representation_apps = grouped_stats[grouped_stats[\"percentage\"]<low_appearance_apps_threshold]\n",
    "\n",
    "random_apps = random.sample(list(low_representation_apps.index), 4)\n",
    "random_apps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c5412",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Let's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "df['eventTimestamp'] = pd.to_numeric(df['eventTimestamp'], errors='coerce')\n",
    "df['eventTimestamp'] = pd.to_datetime(df['eventTimestamp'] / 1000, unit='s')\n",
    "\n",
    "df['year'] = df['eventTimestamp'].dt.year\n",
    "df['month'] = df['eventTimestamp'].dt.month\n",
    "df['day'] = df['eventTimestamp'].dt.day\n",
    "df['hour'] = df['eventTimestamp'].dt.hour\n",
    "df['minute'] = df['eventTimestamp'].dt.minute\n",
    "df['second'] = df['eventTimestamp'].dt.second\n",
    "df['day_of_week'] = df['eventTimestamp'].dt.dayofweek \n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into 90% training and 10% testing sets\n",
    "# and I would randomly select 3 apps \n",
    "train_data, test_data = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "#\n",
    "# I want to make sure we test our model on apps which are not in the training set\n",
    "# For this I would randomly select 4 apps which their % of data is below 5\n",
    "\n",
    "\n",
    "# Filter records associated with new apps from the training set\n",
    "train_data_new_apps = train_data[train_data['bundleId'].isin(random_apps)]\n",
    "\n",
    "# Remove the filtered records from the training set\n",
    "train_data = train_data[~train_data['bundleId'].isin(random_apps)]\n",
    "\n",
    "# Concatenate the existing testing set with records associated with new apps\n",
    "test_data = pd.concat([test_data, train_data_new_apps])\n",
    "\n",
    "# Shuffle the testing set to ensure randomness\n",
    "test_data = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Verify the distribution of apps in the training set\n",
    "print(\"apps in training set:\", train_data['bundleId'].unique())\n",
    "\n",
    "# Verify the distribution of apps in the testing set\n",
    "print(\"apps in testing set:\", test_data['bundleId'].unique())\n",
    "\n",
    "# Check the percentage of data after this modification\n",
    "# Calculate proportions\n",
    "train_proportion = len(train_data) / len(df)\n",
    "test_proportion = len(test_data) / len(df)\n",
    "\n",
    "print(\"Proportion of training data:\", round(train_proportion,4)*100)\n",
    "print(\"Proportion of testing data:\", round(test_proportion,4)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an XGBOOST for this task\n",
    "!pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Select specific columns for training\n",
    "columns_to_use = ['bidFloorPrice', 'year', 'month', 'day', 'hour', 'minute', 'second', 'day_of_week']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[columns_to_use], df['sentPrice'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Evaluate the model accuary on this data:\n",
    "train_data_new_apps[columns_to_use],train_data_new_apps['sentPrice']\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_new_apps = model.predict(train_data_new_apps[columns_to_use])\n",
    "mse_new_apps = mean_squared_error(train_data_new_apps['sentPrice'], y_pred_new_apps)\n",
    "print(\"new apps mse is: \",mse_new_apps)\n",
    "\n",
    "# Optionally, you can also print feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "print(\"Feature Importances:\", feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c14800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets create clusters of the embeddings\n",
    "#def clustter_embeddings(cos_sim_df):\n",
    "    # \n",
    "\n",
    "\n",
    "# Print the modified DataFrame\n",
    "\n",
    "def cluster_apps_by_similarity(cos_sim_df):\n",
    "    clusters = {}  # Dict to store clusters\n",
    "    cluster_counter = 0  # Counter for cluster numbers\n",
    "\n",
    "    for current_app, row in cos_sim_df.iterrows():\n",
    "        if any(current_app in apps for apps in clusters.values()):\n",
    "            continue\n",
    "            \n",
    "        # Find the column (app name) with the highest value in the current row\n",
    "        max_similarity_app = row.idxmax()\n",
    "        \n",
    "        # Check if the app with the highest similarity is already in a cluster\n",
    "        assigned_cluster = None\n",
    "        for cluster_num, apps_in_cluster in clusters.items():\n",
    "            if max_similarity_app in apps_in_cluster:\n",
    "                assigned_cluster = cluster_num\n",
    "                break\n",
    "        \n",
    "        # If the app is already in a cluster, assign the current app to the same cluster\n",
    "        # Otherwise, create a new cluster with both apps\n",
    "        if assigned_cluster is not None:\n",
    "            clusters[assigned_cluster].append(current_app)\n",
    "        else:\n",
    "            cluster_counter += 1\n",
    "            clusters[cluster_counter] = [max_similarity_app, current_app]\n",
    "\n",
    "    return clusters\n",
    "        \n",
    "\n",
    "cluster_apps_by_similarity(cos_sim_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
